<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="“Bert论文笔记, MilkTeaAddicted">
    <meta name="description" content="生活就是奶茶">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>“Bert论文笔记 | MilkTeaAddicted</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="MilkTeaAddicted" type="application/atom+xml">
</head>


<body>
    <!-- 页面点击小红心 -->
    <script type="text/javascript" src="/js/click_show_text.js"></script>

    <!-- <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script> -->


    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">MilkTeaAddicted</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">MilkTeaAddicted</div>
        <div class="logo-desc">
            
            生活就是奶茶
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，返回主页');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/18.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">“Bert论文笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/ML/">
                                <span class="chip bg-color">ML</span>
                            </a>
                        
                            <a href="/tags/NLP/">
                                <span class="chip bg-color">NLP</span>
                            </a>
                        
                            <a href="/tags/Bert/">
                                <span class="chip bg-color">Bert</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/NLP/" class="post-category">
                                NLP
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-05-23
                </div>
                

                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Bert论文笔记"><a href="#Bert论文笔记" class="headerlink" title="Bert论文笔记"></a>Bert论文笔记</h1><p>首先放上论文的地址：</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>NLP中有一个比较重要的任务就是如何将人类的句子和词翻译机器能够理解的表示，从而让机器进行处理。也就是说<strong>如何让机器理解人类语言</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190407192340124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="语言模型发展脉络简要梳理："></p>
<ul>
<li><p>最简单最粗暴的表示方式是one-hot编码，即就像鸢尾花数据集中的one-hot表示，但是如果在词级别上用one-hot进行表示，那么表示词的向量将会非常大而且会过于稀疏，不利于机器处理，并且很难表示词的语义特征。</p>
<p><img src="https://img-blog.csdnimg.cn/20191128112529381.png" alt="img"></p>
</li>
<li><p>后来出现了Word Embedding ，这一方式解决了传统机器学习方法中的特征稀疏问题，它通过把一个词映射到一个低维稠密的语义空间，从而使得相似的词可以共享上下文的信息，从而提升泛化能力。而且通过无监督的训练（比如word2vec和glove方法），从而能够将这些语义知识迁移到数据较少的具体任务上。但是word embeding也有缺点，Word Embedding学到的是一个词的所有语义，比如bank可以是”银行”也可以是”水边。如果一定要用一个固定的向量来编码其语义，那么我们只能把这两个词的语义都编码进去，但是实际一个句子中只有一个语义是合理的，这显然是有问题的。也就是说word Embeding没有考虑到词在具体语境以及中的含义。</p>
<p><img src="https://img-blog.csdnimg.cn/20191024181316166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZfSlVMWV92,size_16,color_FFFFFF,t_70" alt="img"></p>
</li>
<li><p>为了解决句子的在不同语境中的语义不同的问题，可以通过RNN来编码上下文的语义，但是最原始的RNN由于梯度消失和梯度爆炸等问题很难训练，后来引入了LSTM和GRU等模型来解决这个问题。最早的RNN只能用于分类、回归和序列标注等任务，通过引入两个RNN构成的Seq2Seq模型可以解决序列的变换问题。于是到了2017年，Google提出了Transformer模型，引入了Self-Attention。Self-Attention的初衷是为了用Attention替代LSTM，从而可以更好的并行(因为LSTM的时序依赖特效很难并行，而transformer一次可以处理多个词)，从而可以处理更大规模的语料。</p>
<p><img src="https://img-blog.csdnimg.cn/20200818124139104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjk1NTI=,size_16,color_FFFFFF,t_70" alt="img"></p>
</li>
<li><p>于是在transformer的基础上发展出了ELMo（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>）与OpenAI GPT<a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">(Improving Language Understanding by Generative Pre-Training)</a>这两预训练模型。ELMo会根据不同的任务，把上面得到的双向的LSTM的不同层的隐状态组合起来。使得输出的词表示与上下文相关的。可以用于下游(downstream)的特定任务。和ELMo不同，GPT得到的语言模型的参数不是固定的，它会根据特定的任务进行调整(通常是微调)，这样得到的句子表示能更好的适配特定任务。它的思想其实也很简单，使用Transformer来学习一个语言模型，对句子进行无监督的Embedding，然后根据具体任务对Transformer的参数进行微调。从这一点来看，GPT的思想很接近本文要介绍的bert了。</p>
<p><img src="https://img-blog.csdnimg.cn/20190407193050891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="elmo"></p>
<p><img src="http://fancyerii.github.io/img/bert/gpt-2.png" alt="GPT在一些baseline的效果"></p>
</li>
<li><p><strong>一切过往， 皆为序章</strong>：尽管以及有能够考虑到上下文语义信息的模型，但是ELMo和GPT最大的问题就是其语言模型是单向的——我们是根据之前的历史来预测当前词。但是我们不能利用后面的信息。bert之所以state-of-the-art的一点就是其虽然也是基于transformer,但是BERT能够同时利用前后两个方向的信息。主要是源于其预训练的方法的创新。</p>
<blockquote>
<p>注意：即使ELMo训练了双向的两个RNN，但是一个RNN只能看一个方向，因此也是无法”同时”利用前后两个方向的信息的。</p>
</blockquote>
<p><img src="http://fancyerii.github.io/img/bert/bert-1.png"></p>
</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote>
<p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. </p>
</blockquote>
<p>简介：bert首先是一个基于transform的双向编码表示。</p>
<blockquote>
<p>BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. </p>
<p>As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p>
</blockquote>
<p>其次摘要讲了下bert的上游和下游任务中的处理方式：</p>
<p>bert的预训练的策略是输入未标注的文本，bert的所有层中都要同时考虑未标记的文本的上下文，从而得到文本的深度编码表示(即机器能够理解并处理的语义表示)。</p>
<p>因此，预训练好的bert模型可以通过finetuned  ,只用在后面接上一个输出层（应用于下游任务），就可以达到意料之外的效果，因此可以应用到广泛的学习任务中</p>
<blockquote>
<p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p>
</blockquote>
<p>摘要最后讲了下bert的效果：bert在概念上简单，在经验上有力。在11个NLP任务中取得了非常非常不错的成果（效果见下）</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/zingp/1681929/o_200327035726bert_results.png"></p>
<p>以及具体这11个任务：</p>
<blockquote>
<p>MNLI：Multi-Genre Natural Language Inference，给定句子对，判断两个句子是蕴含、矛盾还是中立，三分类<br>QQP：Quora Question Pairs，给定句子对，判断语义是否相似，二分类<br>QNLI：Question Natural Language Inference，给定句子对，判断后者是否是前者的回答，二分类<br>SST-2：Standford Sentiment Treebank，给定单句，判断情感，二分类<br>CoLA：Corpus of Linguistic Acceptability，给定单句，判断是否是一句话？二分类<br>STS-B：Semantic Textual Similarity，给定句子对，判断相似度程度，五分类<br>MRPC：Microsoft Research Paraphrase Corpus，给定句子对，判断语义是否一致，二分类<br>RTE：Recognizing Texual Entailment，给定句子对，判断蕴含性，二分类<br>SQuAD1.1：Stanford Question Answering Dataset，经典阅读理解任务，给定（question，paragraph）对，预测answer在paragraph中的起止位置（span prediction）<br>CoNLL-2003 NER：命名实体识别任务，输入单句，为每个词进行标注<br>SWAG：Situations With Adversarial Generations，原任务是给定一句话，和4个选项，从选项里面找出它的下一句，这里转化为输入pair的分类问题，四分类</p>
</blockquote>
<h2 id="bert的介绍"><a href="#bert的介绍" class="headerlink" title="bert的介绍"></a>bert的介绍</h2><blockquote>
<p>We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. </p>
<p>During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.</p>
</blockquote>
<p>使用bert框架需要两步，即：上游的预训练以及下游的微调步骤</p>
<p>首先gooles对提供海量数据对bert进行了预训练（预训练时是基于未标注的数据），这样我们初始化bert模型的时候就不是随机初始化而是利用了gooles预先训练好的参数和权重，我们需要对bert模型进行微调，以应用到下游的NL任务中（此时我们微调时利用的数据是有标签的数据）</p>
<p>然后论文附了一张图来表示bert在预训练和微调时的区别：</p>
<p><img src="http://imageproxy.chaoxing.com/0x0,q15,jpeg,sWZd92CVrSN4lpFVYERT1CEXxTZp1_7FhIrWlEWu_U2U/http://p.ananas.chaoxing.com/star3/origin/af08075865a6245ea212e2f92d9016e7.png" alt="附图"></p>
<p>预训练时：输入的经过mask处理过后的连续的两句子用一个特殊符号[sep]来进行隔离,[cls]是在没有输入文本示例前加入的特殊符号，某种意义上相当于一个考虑了全文信息的特征表达。bert经过预训练完后能够得到一组权重参数。</p>
<p>微调：微调时的网路结构除了输出层要适应下游的文本的处理任务，其他结构和预训练的时的架构完全一致，微调期间所有参数都要进行微调</p>
<blockquote>
<p>A distinctive feature of BERT is its unified architecture across different tasks.</p>
</blockquote>
<p>Bert在应用到不同任务中时需要进行微调，但是应用到具体任务中时bert的架构都不改变,预训练和微调的方式只有细微的不同。</p>
<h2 id="当前背景介绍"><a href="#当前背景介绍" class="headerlink" title="当前背景介绍"></a>当前背景介绍</h2><blockquote>
<p>We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.</p>
<p>The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).</p>
</blockquote>
<p>作者总结了之前的一些模型的不足；</p>
<p>作者认为当前的处理技术限制了预训练的表示能力，特别是在预训练方式上；</p>
<p>以往的语义处理模型大多是单向的，只考虑上文或者只考虑下文。</p>
<p>比如在早期的GPT模型中，作者只考虑前文，这使得文章中每个taken都只参考Transformer的自注意层之前的token.也就是说以往的语义模型单向（即使是训练了双向RNN的ELMo，但是一个RNN只能关注文本的一个方向，无法像摘要中提到的那样，每一个层都利用文本前后的信息）</p>
<blockquote>
<p>In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. </p>
<p>BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked arXiv:1810.04805v2 [cs.CL] 24 May 2019 word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows:</p>
</blockquote>
<p>于是为了改进前人的语义模型，使得模型能够同时利用上下文的信息，Bert改进了预训练手段。比较创新的一点就是MLM的引入</p>
<h2 id="bert的预训练"><a href="#bert的预训练" class="headerlink" title="bert的预训练"></a>bert的预训练</h2><p>预训练的创新点在于如何让bert能够同时关注句子前后两个方向的信息</p>
<p>具体的实现方式可以如此描述：</p>
<p>bert 的模型根据完形填空这个任务的启发，通过使用一种叫MLM的的方式对文本进行预训练。</p>
<h3 id="预训练任务1-Masked-LM"><a href="#预训练任务1-Masked-LM" class="headerlink" title="预训练任务1 Masked LM"></a>预训练任务1 Masked LM</h3><p>（这里看论文没有理解，于是看了下源码的自述文件）</p>
<blockquote>
<p>在原始的预处理代码中，我们随机选择要屏蔽的单词级别的token(也就是像完形填空一样随机地盖住一些词)。例如：</p>
<pre><code>Input Text（原文本）: the man jumped up , put his basket on phil ##am ##mon &#39; s head` 
`Original Masked Input（经过遮盖后的输出）: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon &#39; s head</code></pre>
</blockquote>
<p>bert在预训练的时候会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。</p>
<p>但是这有一个问题：在Mask 时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。</p>
<blockquote>
<p>（原文本）: the man jumped up , put his basket on phil ##am ##mon ‘ s head</p>
<p>（经过遮盖后的输出）: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ‘ s head</p>
</blockquote>
<p>该如何理解这句话？还是以上述文本作为例子，（the）在这一轮预训练的时候以15%的概率被mask掉了，可能下一轮(the)这个词比较倒霉，还被mask掉，可能这个词一直运气都比较背，每轮都被mask，这样bert就只知道（the）位置上有个空缺，但是不知道这个词到底是什么，为了避免这种事情的发生，bert在mask词的时候采用了一种非常不错的机制：</p>
<p>如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行：</p>
<blockquote>
<ul>
<li>80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK]（这样一个词真正被mask的可能降低到15%*80%，在这种概率下，实际情况中基本上不会出现某个词一直被mask无法被学习到的情况）</li>
<li>10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple（相当于加入噪音，提高泛化性）</li>
<li>10%的概率替换成它本身，比如my dog is hairy → my dog is hairy</li>
</ul>
</blockquote>
<p>这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。</p>
<h3 id="预训练任务2-Next-Sentence-Prediction-NSP"><a href="#预训练任务2-Next-Sentence-Prediction-NSP" class="headerlink" title="预训练任务2 Next Sentence Prediction (NSP)"></a>预训练任务2 Next Sentence Prediction (NSP)</h3><p>许多重要的下游任务，如问答（QA）和自然语言推理（NLI）都是建立在理解两个句子之间关系的基础上的，而语言建模并不能直接捕捉到这一点。</p>
<p>也就是说为了适应某些下游任务的需要，因此BERT还增加了一个新的任务——预测两个句子是否有关联关系，这是一种Multi-Task Learing。</p>
<p>bert的预训练时选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。</p>
<blockquote>
<p>50%: [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</p>
<p>另外50%：[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</p>
</blockquote>
<h3 id="预训练流程"><a href="#预训练流程" class="headerlink" title="预训练流程"></a>预训练流程</h3><blockquote>
<p> The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.</p>
<p>使用BooksCorpus(800万字)(Zhu et al.， 2015)和English Wikipedia(2500万字)。对于Wikipedia，我们只提取文本段落，而忽略列表、表格和标题。使用文档级语料库而不是打乱的句子级语料库(如Billion Word Benchmark (Chelba et al.， 2013))来提取长连续序列是至关重要的。</p>
</blockquote>
<p>用BooksCorpus（800M词）+ 英文Wikipedia（2500M词）作为语料，作者强调使用document-level的语料比随机shuffle句子的语料（比如Billion Word Benchmark）要好，因为这样才能抽取出连续的长句子，才好用于BERT的数据构造。</p>
<p>bert模型是非常大的，所以train来非常消耗资源。</p>
<p>都是训练了4天,BASE版本的BERT，用16块TPU，LARGE版本的BERT，用64块TPU，只能说谷歌爸爸真的非常有钱。</p>
<p><img src="https://img-blog.csdnimg.cn/20190407194312495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="参数对比以及预训练时长以及配置"></p>
<h2 id="bert的微调"><a href="#bert的微调" class="headerlink" title="bert的微调"></a>bert的微调</h2><blockquote>
<p>For each task, we simply plug in the task specific inputs and outputs into BERT and fine tune all the parameters end-to-end. </p>
<p>对于每个任务，我们只需将特定于任务的输入和输出插入BERT，并端到端调整所有参数。</p>
</blockquote>
<p>普通人很难训练一个标准的bert模型，据说8个GPU一起训练需要训练一年左右。但是谷歌已经预训练好了bert模型并且提供了开源使用<a target="_blank" rel="noopener" href="https://github.com/google-research/bert">（代码地址）</a>，所以如果我们要使用bert模型，只需对其进行微调，应用到具体任务中即可。</p>
<h3 id="论文中提及的应对四种不同任务的微调方式"><a href="#论文中提及的应对四种不同任务的微调方式" class="headerlink" title="论文中提及的应对四种不同任务的微调方式"></a>论文中提及的应对四种不同任务的微调方式</h3><p>论文中探讨了4用bert处理的任务，论述了bert的使用。</p>
<p><img src="http://fancyerii.github.io/img/bert/bert-3.png"></p>
<ul>
<li><p>对于普通的分类任务，输入是一个序列，如图中右上所示，所有的Token都是属于同一个Segment(Id=0)，我们用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，用分类的数据来进行Fine-Tuning(<strong>即需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</strong>)。</p>
</li>
<li><p>对于相似度计算等输入为两个序列的任务（句子对关系任务）），过程如图左上所示。<strong>两个序列的Token对应不同的Segment(Id=0/1)。我们也是用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，然后用分类数据进行Fine-Tuning</strong>。</p>
</li>
<li><p>对于序列标注（ SQuAD v2.0），比如命名实体识别，输入是一个句子(Token序列)，除了[CLS]和[SEP]的每个时刻都会有输出的Tag，比如B-PER表示人名的开始，本章的序列标注部分已经介绍过怎么把NER变成序列标注的问题了，这里不再赘述。然后用输出的Tag来进行Fine-Tuning，过程如图右下所示。<strong>即对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</strong>；</p>
</li>
<li><p>第四类是问答类问题，比如SQuAD v1.1数据集，输入是一个问题和一段很长的包含答案的文字(Paragraph)，输出在这段文字里找到问题的答案。<strong>即将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</strong></p>
</li>
</ul>
<h3 id="应用bert前应做的数据预处理"><a href="#应用bert前应做的数据预处理" class="headerlink" title="应用bert前应做的数据预处理"></a>应用bert前应做的数据预处理</h3><p>由于bert是基于transformer的编码器，所以数据预处理要符合tramformer的需求</p>
<p>这里引用了<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37763870/article/details/104452213">自然语言处理——BERT情感分类实战(一)之预处理</a></p>
<p>其中预处理有如下六步</p>
<p><img src="https://img-blog.csdnimg.cn/20200222225157335.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzc2Mzg3MA==,size_16,color_FFFFFF,t_70" alt="文本数据喂入bert前因做的六部预处理工作"></p>
<ul>
<li><p>step1<strong>分词</strong>：将文本的词按照单词级别进行划分，注意这里分词的规则：<img src="https://img-blog.csdnimg.cn/20200223122520237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzc2Mzg3MA==,size_16,color_FFFFFF,t_70" alt="示例"></p>
</li>
<li><p>step2<strong>对文本划分句子以及标注</strong>：为句首添加[CLS]标志，然后用[SEP]划分每个字句</p>
<p><img src="https://img-blog.csdnimg.cn/20200223122958594.png"></p>
</li>
<li><p>step3<strong>用[PAD]填充句子</strong>：因为输入的每个句子要保证序列长度要一致，所以如果我定义喂入bert的序列长度为20，则如果现在句子长度（加上cls和sep后的长度）没有20,则要用[pad]进行填充，保证输入进入的序列长度是20,这从引用者的代码便可以看出</p>
<pre><code class="python">padded_tokens = tokens + [&#39;[PAD]&#39; for _ in range(20 - len(tokens))]
print(padded_tokens)
#效果如下</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20200223123621517.png" alt="填充效果"></p>
</li>
</ul>
<ul>
<li><p>step4<strong>注意力mask编码</strong>：即有单词的为1，用[PAD]填充的为0，让bert知道句子的什么地方有语义什么地方没有</p>
<p><img src="https://img-blog.csdnimg.cn/20200223124312192.png" alt="img"></p>
</li>
<li><p>step5<strong>分段segment编码</strong>:用于区分不同的句子，我们这里只有一个句子，故全设为0</p>
<p><img src="https://img-blog.csdnimg.cn/20200223124943164.png" alt="img"></p>
</li>
<li><p>step6<strong>把token转化为id</strong>:文本分词后创建一个包含对应字 id的字典，这一步则是要查到每个单词对应的序号，转化为id</p>
<p><img src="https://img-blog.csdnimg.cn/20200223125408549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzc2Mzg3MA==,size_16,color_FFFFFF,t_70" alt="img"></p>
</li>
</ul>
<p>最后我们要把得到了mask编码、segment编码、taken_id一起喂入bert</p>
<p><img src="https://img-blog.csdnimg.cn/20200223125937384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzc2Mzg3MA==,size_16,color_FFFFFF,t_70" alt="首先三者转化为tensor"></p>
<p><img src="https://img-blog.csdnimg.cn/20200223130559772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzc2Mzg3MA==,size_16,color_FFFFFF,t_70" alt="然后把三者一起喂进bert里，bert会返回我们下游任务需要的cls和hindden_reps的tensor表示"></p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><blockquote>
<p>BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, </p>
<p>we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”</p>
</blockquote>
<p>bert的模型架构是参考了Vaswani et al. (2017)的多层双向的 Transformer 编码器，因为bert对此实现基本上和原参考一致，而且transformer在NLP任务中使用比较广泛，所以这篇文章就不再细讲Vaswani et al. (2017)中编码器的实现（bert中的’T’ 的实现）</p>
<blockquote>
<p>In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. 3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).</p>
</blockquote>
<p>在介绍bert模型的架构时作者做出如下定义：</p>
<ul>
<li>作者直接定义一个Transformer blocks作为网络中的一层（L）,（模型将将堆叠作者定义的L）。</li>
<li>隐藏层神经元规模用H表示（表示编码的维度）</li>
<li>A表示self-attention heads的数量；</li>
</ul>
<p>作者就L，H，A的不同进行了效果比对，发现参数越多规模越大效果越好</p>
<p><img src="http://fancyerii.github.io/img/bert/bert-8.png" alt="img"></p>
<h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p>在GLUE评测平台上的结果如下图所示，我们可以发现BERT比之前最好的OpenAI GPT还提高了很多</p>
<p><img src="http://fancyerii.github.io/img/bert/bert-4.png" alt="img"></p>
<p>在SQuAD数据集上，BERT之前最好的结果F1=89.3%，而7个BERT的ensembling能达到93.2%的F1得分</p>
<p><img src="http://fancyerii.github.io/img/bert/bert-5.png" alt="img"></p>
<p>bert模型的创新点在于Mask LM和预测句子关系的Multi-Task Learning。为了知道每个改动的贡献，文章做了如下的对照(Ablation)实验。</p>
<p>如下图所示，这里测试用的是小参数的一个BERT参考模型；</p>
<ul>
<li><p>No NSP是没有预测句子关系(只有Mask LM)的BERT模型；</p>
</li>
<li><p>-LTR &amp; No NSP基本等同于OpenAI GPT，它是基于Transoformer的从左到右的普通语言模型；</p>
</li>
<li><p>而最后一行+BiLSTM是指在Fine-Tuning OpenAI GPT的时候多加一个双向LSTM层(通常的Fine-Tuning都是只有一个线性层)。</p>
<p><img src="http://fancyerii.github.io/img/bert/bert-7.png" alt="img"></p>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>bert的优点就不用说了11个NLP任务中屠榜</p>
<p>但是作者在论文中也提到了bert的缺点：</p>
<ul>
<li>[MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现</li>
<li>每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）（<strong>虽然预训练慢但是应用起来准确率高，对有钱有设备的谷歌来说都是小问题</strong>）</li>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制（见上数据预处理），适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="http://fancyerii.github.io/2019/03/09/bert-theory/">BERT模型详解- 李理的博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Peter_Changyb/article/details/109162774">NLP中的经典BERT模型详解(最全面)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Magical_Bubble/article/details/89514057">BERT解读（论文 + TensorFlow源码）</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zingp/p/13849679.html">BERT模型详解- ZingpLiu - 博客园</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37763870/article/details/104452213">自然语言处理——BERT情感分类实战(一)之预处理</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1P4411F77q?t=2050">汉语自然语言处理-从零解读碾压循环神经网络的transformer模型(一)-注意力机制-位置编码-attention is all you need</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Mt411J734?p=2&t=1866">汉语自然语言处理-BERT的解读语言模型预训练-实践应用-transformer模型(二)-语料预处理-情感分析分类-数据增强-解决过拟合问题</a></p>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">晓沐</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://sweetheart.nefu.site/2021/05/22/%E2%80%9CBert%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">http://sweetheart.nefu.site/2021/05/22/%E2%80%9CBert%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">晓沐</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/ML/">
                                    <span class="chip bg-color">ML</span>
                                </a>
                            
                                <a href="/tags/NLP/">
                                    <span class="chip bg-color">NLP</span>
                                </a>
                            
                                <a href="/tags/Bert/">
                                    <span class="chip bg-color">Bert</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'Om52MKm7YQjVc1iJPsts4J60-gzGzoHsz',
        appKey: 'uK5GGz7Gau2tufy7gVIuueMN',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '有什么想说的欢迎留言鸭~'
    });
</script>

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2021/05/22/%E2%80%9CBert%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/18.jpg" class="responsive-img" alt="“Bert论文笔记">
                        
                        <span class="card-title">“Bert论文笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Bert论文笔记首先放上论文的地址：
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
背景介绍NLP中有一个比较重要的任务就是
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/NLP/" class="post-category">
                                    NLP
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/ML/">
                        <span class="chip bg-color">ML</span>
                    </a>
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                    <a href="/tags/Bert/">
                        <span class="chip bg-color">Bert</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/04/17/%E4%BD%BF%E7%94%A8Python%E8%AF%AD%E8%A8%80%E5%88%B7%E8%93%9D%E6%A1%A5%E6%9D%AF%E5%8E%86%E5%B9%B4%E7%9C%9F%E9%A2%98/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/7.jpg" class="responsive-img" alt="使用Python语言刷蓝桥杯历年真题">
                        
                        <span class="card-title">使用Python语言刷蓝桥杯历年真题</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            用Python来刷蓝桥杯历年真题吧其实看这发布时间就知道应该是就是考前抱佛脚考前演练
11届蓝桥杯 Python组11届是蓝桥杯第一次开Python赛道，之前刷题都是按照C/C艹组的难度来刷的题目
看看第一届Python组会有多难吧
试题A
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/" class="post-category">
                                    蓝桥杯
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Python/">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="/about" target="_blank">晓沐</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/MilkTeaAddicted" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3364268153@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>





    <a href="https://twitter.com/MilkTeaAddicted" class="tooltipped" target="_blank" data-tooltip="关注我的Twitter: https://twitter.com/MilkTeaAddicted" data-position="top" data-delay="50">
        <i class="fab fa-twitter"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=3364268153" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 3364268153" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
